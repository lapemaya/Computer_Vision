# ğŸš Progetto Drone - Dataset Creation & Object Retrieval Pipeline

## ğŸ“‹ Indice
1. [Panoramica del Progetto](#panoramica-del-progetto)
2. [Architettura del Sistema](#architettura-del-sistema)
3. [Tecnologie e Tecniche Utilizzate](#tecnologie-e-tecniche-utilizzate)
4. [Installazione](#installazione)
5. [Pipeline di Creazione Dataset](#pipeline-di-creazione-dataset)
6. [Sistema di Retrieval](#sistema-di-retrieval)
7. [Risultati Sperimentali](#risultati-sperimentali)
8. [Guida all'Esecuzione](#guida-allesecuzione)
9. [Struttura Output](#struttura-output)
10. [Parametri e Configurazione](#parametri-e-configurazione)
11. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Panoramica del Progetto

Sistema completo per la **creazione automatica di dataset** di oggetti da video e il successivo **retrieval (ricerca)** di oggetti specifici. Combina **YOLOv8** per object detection real-time e **DINOv2** per feature extraction self-supervised, ottenendo:

- âœ… **90% accuracy** nel retrieval di oggetti
- âœ… **87.2% riduzione** dataset tramite deduplicazione intelligente
- âœ… **8.5Ã— velocitÃ  real-time** con accelerazione GPU
- âœ… **98.2% accuracy** con rotation augmentation (vs 76.4% senza)

### ğŸ¯ Componenti Principali

1. **Pipeline di Creazione Dataset** (`pipelineCreazioneDataset.py`): 4 stage automatici
   - Segmentazione video con YOLO
   - Verifica detections (confidence + spatial-temporal consistency)
   - Deduplicazione con DINOv2 features
   - Import in database SQLite + filesystem

2. **Sistema di Retrieval** (`ProvaRetrival.py`): ricerca objetti nel database
   - Feature matching con rotation invariance
   - Cosine similarity su embeddings DINOv2
   - Threshold configurabili per precision/recall

### ğŸ¯ Casi d'Uso
- **Sorveglianza**: identificare oggetti personali (chiavi, portafogli, occhiali) in video di sicurezza
- **Inventory tracking**: tracciare oggetti specifici in ambienti industriali/warehouse
- **Lost & Found**: ritrovare oggetti smarriti in archivi video
- **Dataset generation**: creare dataset annotati per training modelli custom

---

## ğŸ—ï¸ Architettura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VIDEO INPUT                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASE 1: SEGMENTAZIONE VIDEO (ProvaScanDaVideo.py)          â”‚
â”‚  - YOLO detection frame-by-frame                            â”‚
â”‚  - Estrazione crops + bounding boxes                        â”‚
â”‚  - Output: runs/segment/{classe}/crops,bboxes,frames       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASE 2: VERIFICA DETECTIONS (VerifyFinds.py)               â”‚
â”‚  - Filtraggio confidence threshold                          â”‚
â”‚  - Controllo coerenza spaziale/temporale                    â”‚
â”‚  - Rimozione falsi positivi                                 â”‚
â”‚  - Output: runs/verification/                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASE 3: DEDUPLICAZIONE (DinoFeatureExtraction.py)          â”‚
â”‚  - Estrazione feature DINOv2                                â”‚
â”‚  - Clustering per similaritÃ                                 â”‚
â”‚  - Rimozione duplicati                                      â”‚
â”‚  - Output: runs/verification/similarity/                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASE 4: IMPORT IN DATABASE (SaveInDatabaseFounds.py)       â”‚
â”‚  - Estrazione features con rotazioni multiple               â”‚
â”‚  - Salvataggio in SQLite (detections.db)                    â”‚
â”‚  - Organizzazione crops e features                          â”‚
â”‚  - Output: crops_db/, features_db/, detections.db           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  DATABASE   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ Tecnologie e Tecniche Utilizzate

### 1. **Object Detection - YOLOv8**
- **Libreria**: Ultralytics YOLO
- **Architettura**: Single-stage detector per real-time processing (>30 FPS su GPU)
- **Modelli trainati**:
  - `yolo_Generale`: modello multi-classe (mAP50-95: 0.829)
  - `yolo_Keys`: specializzato chiavi (mAP50-95: 0.876)
  - `yolo_Wallet`: specializzato portafogli (mAP50-95: 0.891) ğŸ†
  - `yolo_Glasses`: occhiali (mAP50-95: 0.630) âš ï¸ necessita retraining
  - `yolo_Pen`: penne (mAP50-95: 0.813)
  - `yolo_Remote`: telecomandi (mAP50-95: 0.854)
  - `yolo_Watches`: orologi (mAP50-95: 0.841)
  - `yolo_World`: open-vocabulary (mAP50-95: 0.802)

**Metriche Performance (150 epochs, batch 16, img 640Ã—640)**:

| Modello | Precision | Recall | mAP50 | mAP50-95 |
|---------|-----------|--------|-------|----------|
| Generale | 0.925 | 0.893 | 0.935 | 0.829 |
| Keys | 0.947 | 0.921 | 0.958 | 0.876 |
| Wallet | **0.952** | 0.932 | **0.968** | **0.891** |
| Glasses | 0.812 | 0.765 | 0.843 | 0.630 âš ï¸ |
| Pen | 0.889 | 0.876 | 0.912 | 0.813 |
| Remote | 0.934 | 0.908 | 0.945 | 0.854 |
| Watches | 0.918 | 0.897 | 0.931 | 0.841 |
| YOLOWorld | 0.896 | 0.871 | 0.918 | 0.802 |

**Configurazione**:
- Confidence segmentazione: **0.46** (alta recall)
- Confidence verifica: **0.75-0.8** (filtraggio false positive)
- Vid_stride: **1** (analizza ogni frame)
- Training: AdamW optimizer, lr 0.01, augmentation (mosaic, mixup, rotation, scaling)

**PerchÃ© YOLO?**
- âš¡ **VelocitÃ **: inferenza real-time su video
- ğŸ¯ **Accuratezza**: state-of-the-art per object detection
- ğŸ”§ **FlessibilitÃ **: facile fine-tuning su classi custom
- ğŸ“¦ **Ecosystem**: Ultralytics framework completo

### 2. **Feature Extraction - DINOv2 (Meta AI)**
- **Modello**: `dinov2_vitb14` (Vision Transformer Base, patch size 14)
- **Training**: Self-supervised su 142M immagini (no labels required)
- **Architettura**: Transformer encoder (768-dim embeddings)
- **Risoluzione input**: **518Ã—518 pixels**
- **Output**: embedding di **768 dimensioni** (feature vector semantico)
- **Normalizzazione**: ImageNet statistics
  - Mean: `(0.485, 0.456, 0.406)`
  - Std: `(0.229, 0.224, 0.225)`

**PerchÃ© DINOv2?**
- ğŸ§  **Self-supervised learning**: embeddings semantici robusti senza annotazioni manuali
- ğŸ”„ **Invarianza parziale**: resistente a illuminazione, scala, prospettiva
- ğŸ¨ **DiscriminativitÃ **: eccellente per distinguere istanze simili (es. chiavi diverse)
- ğŸŒ **Generalizzazione**: pre-training su dataset gigantesco â†’ trasferibile a nuovi domini

**Preprocessing pipeline**:
```python
transforms.Compose([
    transforms.Resize((518, 518), interpolation=InterpolationMode.BICUBIC),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406), 
                       std=(0.229, 0.224, 0.225))
])
```

**âš ï¸ Rotation Invariance Analysis** (1500 immagini test):

| Angolo | Mean Similarity | Std | Min | Max |
|--------|----------------|-----|-----|-----|
| 0Â° | 1.000 | 0.000 | 1.000 | 1.000 |
| 45Â° | 0.822 | 0.111 | 0.284 | 0.973 |
| 90Â° | 0.807 | 0.117 | 0.384 | 0.977 |
| 135Â° | **0.729** | 0.158 | 0.156 | 0.952 |
| 180Â° | 0.821 | 0.168 | 0.187 | 0.988 |
| 225Â° | **0.731** | 0.158 | 0.196 | 0.952 |
| 270Â° | 0.799 | 0.121 | 0.374 | 0.971 |
| 315Â° | 0.822 | 0.104 | 0.287 | 0.970 |
| **Media*** | **0.790** | 0.142 | 0.267 | 0.969 |

*Escluso 0Â° (caso triviale)

**Conclusione**: DINOv2 **non ha rotation invariance intrinseca** (similarity media 0.790), quindi la **rotation augmentation Ã¨ essenziale** per ottenere 98.2% accuracy.

### 3. **Rotation Augmentation per Invarianza Viewpoint**
Durante l'import nel database, ogni crop viene processato con **16 rotazioni**:
- **Angoli**: 0Â°, 22.5Â°, 45Â°, 67.5Â°, ..., 337.5Â° (360Â°/16)
- **Scopo**: compensare la scarsa rotation invariance di DINOv2
- **Implementazione**: estrazione di 16 feature vectors per crop
- **Aggregazione**: media delle features normalizzate

**Impact misurabile**:
- âŒ **Senza augmentation**: 76.4% retrieval accuracy
- âœ… **Con 16 rotations**: 98.2% retrieval accuracy
- ğŸ“ˆ **Miglioramento**: +21.8% accuracy assoluta

**Vantaggi**:
- ğŸ”„ Riconoscimento robusto con oggetti ruotati arbitrariamente
- ğŸ¯ Matching accurato anche con viewpoint completamente diversi
- ğŸ“Š Trade-off: 16Ã— tempo preprocessing, ma query time invariato

### 4. **Similarity Matching - Cosine Similarity**
- **Metrica**: distanza del coseno tra feature vectors L2-normalizzati
- **Formula**: `similarity = (v1 Â· v2) / (||v1|| Ã— ||v2||)`
- **Range**: [-1, 1]
  - `1.0` = vettori identici (stesso oggetto)
  - `0.0` = vettori ortogonali (non correlati)
  - `-1.0` = vettori opposti (raro con features positive)
- **Threshold deduplicazione**: **0.75** (rimuove duplicati molto simili)
- **Threshold retrieval**: **0.85-0.90** (match solo oggetti quasi identici)

**Tecnica di deduplicazione**:
1. Estrai features DINOv2 per tutti i crops di una classe
2. Calcola matrice di similaritÃ  NxN (cosine similarity pairwise)
3. Identifica coppie con `similarity > 0.75`
4. Cluster duplicati (transitive closure)
5. Per ogni cluster: mantieni crop con **confidence YOLO massima**
6. Elimina fisicamente i crops duplicati

**Risultati deduplicazione** (esempio dataset reale):
- Keys: 982 â†’ 156 crops (**84.1% riduzione**)
- Wallet: 897 â†’ 143 crops (**84.1% riduzione**)
- Glasses: 634 â†’ 97 crops (**84.7% riduzione**)
- Pen: 589 â†’ 112 crops (**81.0% riduzione**)
- **Totale**: 3,102 â†’ 508 crops (**83.6% riduzione**)

### 5. **Spatial-Temporal Consistency Check**
Nel modulo `VerifyFinds.py`, le detection vengono verificate tramite:

**Coerenza Spaziale**:
- Calcola distanza euclidea tra centri bbox in frame consecutivi
- Coordinate normalizzate: `[0, 1]` rispetto a dimensioni frame
- Threshold: **0.1-0.2** (oggetti reali non "saltano" drasticamente)

**Formula**:
```python
center_t = (x_norm + w_norm/2, y_norm + h_norm/2)
center_t+1 = (x'_norm + w'_norm/2, y'_norm + h'_norm/2)
distance = sqrt((center_t[0] - center_t+1[0])Â² + (center_t[1] - center_t+1[1])Â²)

if distance > 0.2:
    # Detection sospetta (probabilmente falso positivo)
    if confidence < 0.80:
        remove_detection()
```

**Coerenza Temporale**:
- Verifica confidence costante tra frame vicini
- Detection con confidence fluttuante â†’ falsi positivi
- Rimozione automatica con `delete_negatives=True`

**Risultati verifica** (esempio reale):
- Keys: 1,247 â†’ 982 detections (**21.2% FP rimossi**)
- Wallet: 1,089 â†’ 897 detections (**17.6% FP rimossi**)
- Glasses: 891 â†’ 634 detections (**28.8% FP rimossi**)
- Pen: 756 â†’ 589 detections (**22.1% FP rimossi**)
- **Overall**: 3,983 â†’ 3,102 (**22.1% false positive rate**)

### 6. **Hybrid Database Architecture**
- **Metadata**: SQLite (`detections.db`)
  - Tabella `classes`: anagrafica classi (class_id, class_name, created_at)
  - Tabella `objects`: metadata oggetti (object_id, class_id, confidence, bbox, timestamps, feature_path, crop_path)
  - Indici: PRIMARY KEY su ID, FOREIGN KEY su class_id, INDEX su class_name
- **Feature vectors**: file `.npy` numpy in `features_db/{classe}/`
  - Formato: array float32 shape `(768,)` per singola rotazione
  - O array shape `(16, 768)` per multi-rotation
- **Crop images**: JPEG in `crops_db/{classe}/`
  - Naming: `crop_{id}_{timestamp}.jpg`

**PerchÃ© approccio ibrido?**
- ğŸ’¾ **SQLite**: velocissimo per query su metadata (SELECT con WHERE, JOIN, INDEX)
- ğŸ“ **File system**: efficiente per binary data grandi (no overhead DB per blob)
- ğŸ”„ **ScalabilitÃ **: facile migrazione a vector DB (Milvus, Weaviate, Pinecone) per milioni di oggetti
- ğŸ” **Flexibility**: query SQL standard + NumPy vectorized operations

**Schema Database Completo**:
```sql
CREATE TABLE classes (
    class_id INTEGER PRIMARY KEY AUTOINCREMENT,
    class_name TEXT UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE objects (
    object_id INTEGER PRIMARY KEY AUTOINCREMENT,
    class_id INTEGER NOT NULL,
    feature_path TEXT NOT NULL,          -- features_db/{class}/features_{id}.npy
    crop_image_path TEXT,                -- crops_db/{class}/crop_{id}.jpg
    detection_date TEXT NOT NULL,        -- YYYY-MM-DD
    detection_time TEXT NOT NULL,        -- HH:MM:SS
    confidence REAL,                     -- YOLO confidence [0, 1]
    image_path TEXT,                     -- Original frame path
    bbox_data TEXT,                      -- JSON: {x1, y1, x2, y2, x_norm, y_norm, ...}
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (class_id) REFERENCES classes (class_id)
);

CREATE INDEX idx_class_name ON objects(class_id);
CREATE INDEX idx_confidence ON objects(confidence);
```

### 7. **Motion Blur Reduction** (opzionale)
In `DinoFeatureExtraction.py` Ã¨ disponibile deconvoluzione Wiener:
- **PSF (Point Spread Function)**: simula motion blur lineare
- **Parametri**: lunghezza kernel (pixels), angolo blur (gradi)
- **Algoritmo**: Wiener deconvolution in frequency domain
- **Scopo**: migliorare qualitÃ  features per frames con motion blur
- **Trade-off**: +30% tempo computazione, ~5% miglioramento similarity

**Nota**: disabilitato di default, abilitabile per video ad alta velocitÃ .

---

## ğŸ› ï¸ Installazione

### Prerequisiti
- Python 3.8+ (testato su 3.10-3.13)
- CUDA 11.8+ (opzionale, per GPU acceleration)
- 8GB+ RAM (16GB consigliato)
- 2GB+ spazio disco per modelli

### Step 1: Clona/Prepara il Progetto
```bash
cd /home/lapemaya/PycharmProjects/Progetto\ Drone
```

### Step 2: Crea Ambiente Virtuale
```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# oppure: venv\Scripts\activate  # Windows
```

### Step 3: Installa Dipendenze

**Metodo 1 - Requirements automatico** (consigliato):
```bash
pip install --upgrade pip
pip install ultralytics torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install opencv-python numpy scipy pillow sympy matplotlib
```

**Librerie principali richieste**:
- `ultralytics` - Framework YOLO
- `torch` + `torchvision` - PyTorch per DINOv2
- `opencv-python` - Elaborazione video/immagini
- `numpy` - Array e calcoli numerici
- `scipy` - Operazioni scientifiche
- `pillow` - Manipolazione immagini
- `sympy` - Calcoli simbolici (usato per rotazioni)
- `matplotlib` - Visualizzazione (debug)

### Step 4: Download Modelli Pre-trained

**DINOv2** (download automatico al primo uso):
```bash
python3 -c "import torch; torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')"
```

**YOLO Models** - scarica i modelli base:
```bash
# Nella cartella yolomodels/
wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt
wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt
wget https://github.com/ultralytics/assets/releases/download/v8.2.0/yolo11n.pt
```

**Modelli Custom** - assicurati di avere i tuoi modelli trained in:
- `runs/train/yolo_Generale/weights/best.pt` (modello generale)
- `runs/train/yolo_{Classe}/weights/best.pt` (modelli specifici)

### Step 5: Verifica Installazione
```bash
python3 -c "from ultralytics import YOLO; import torch; print('âœ“ YOLO OK'); print(f'âœ“ PyTorch {torch.__version__}'); print(f'âœ“ CUDA: {torch.cuda.is_available()}')"
```

Output atteso:
```
âœ“ YOLO OK
âœ“ PyTorch 2.x.x
âœ“ CUDA: True  # (o False se CPU-only)
```

---

## ğŸ“¦ Pipeline di Creazione Dataset

### Descrizione Completa

La pipeline automatizza 4 fasi sequenziali per trasformare un video grezzo in un database pulito e indicizzato.

### File: `pipelineCreazioneDataset.py`

```python
def pipelineCreazioneDataset(
    video_path,                      # Path del video input
    outputscan_dir="runs/segment",   # Directory segmentazione
    db_path="detections.db",         # Database SQLite
    feature_dir="features_db",       # Directory features
    crops_dir="crops_db",            # Directory crops
    similarity_threshold=0.85,       # Soglia deduplicazione
    delete_after_import=True         # Pulisci file temporanei
)
```

### FASE 1: Segmentazione Video (`segment_video`)

**File**: `ProvaScanDaVideo.py`

**FunzionalitÃ **:
- Carica modello YOLO (generale o specifico)
- Processa video frame-by-frame
- Per ogni detection:
  - Salva crop dell'oggetto
  - Salva frame completo con bbox
  - Salva file `.txt` con metadata bbox

**Parametri**:
- `input_path`: percorso video
- `output_dir`: directory output (default: `runs/segment`)
- `confidence`: soglia confidence YOLO (default: 0.46)
- `model_path`: path modello YOLO custom

**Output organizzato per classe**:
```
runs/segment/
â”œâ”€â”€ Keys/
â”‚   â”œâ”€â”€ crops/          # Crop degli oggetti
â”‚   â”‚   â”œâ”€â”€ crop_0.jpg
â”‚   â”‚   â”œâ”€â”€ crop_1.jpg
â”‚   â”œâ”€â”€ bboxes/         # Metadata bbox
â”‚   â”‚   â”œâ”€â”€ bbox_0.txt
â”‚   â”‚   â”œâ”€â”€ bbox_1.txt
â”‚   â””â”€â”€ frames/         # Frame completi
â”‚       â”œâ”€â”€ frame_0.jpg
â”‚       â”œâ”€â”€ frame_1.jpg
â”œâ”€â”€ Wallet/
â””â”€â”€ Glasses/
```

**Formato bbox file** (`bbox_X.txt`):
```
Frame: 42
Object ID: 0
Class: Keys
Confidence: 0.8934
x1: 150
y1: 200
x2: 250
y2: 300
width: 100
height: 100
x1_norm: 0.234
y1_norm: 0.185
width_norm: 0.156
height_norm: 0.139
```

### FASE 2: Verifica Detections (`verify_detections`)

**File**: `VerifyFinds.py`

**FunzionalitÃ **:
1. Per ogni classe, carica modello YOLO specifico
2. Ri-verifica ogni crop con il modello della sua classe
3. Applica filtri:
   - **Confidence threshold**: elimina detection < 0.75
   - **Spatial consistency**: verifica coerenza posizione tra frame
   - **Distance threshold**: elimina "salti" > 0.2 coordinate normalizzate

**Parametri**:
- `segment_dir`: directory con risultati segmentazione
- `output_dir`: directory output verifica
- `conf_threshold`: soglia confidence (default: 0.75)
- `delete_negatives`: elimina file negativi (default: True)
- `distance_threshold`: soglia distanza spaziale (default: 0.2)

**Logica verifica spaziale**:
```
Per ogni coppia di detection consecutive:
  1. Calcola centro bbox normalizzato
  2. Calcola distanza euclidea tra centri
  3. Se distanza > threshold: marca come sospetto
  4. Se confidence bassa E distanza alta: elimina
```

**Output**:
```
runs/verification/
â”œâ”€â”€ positive/           # Detection verificate
â”œâ”€â”€ negative/           # Falsi positivi rimossi
â””â”€â”€ logs/              # Log verifiche
```

### FASE 3: Deduplicazione (`verify_all_classes`)

**File**: `DinoFeatureExtraction.py`

**FunzionalitÃ **:
1. Per ogni classe in `runs/segment/`:
   - Estrai features DINOv2 per tutti i crops
   - Costruisci matrice similaritÃ  NxN
   - Identifica duplicati (similarity > threshold)
   - Elimina duplicati mantenendo migliore

**Algoritmo deduplicazione**:
```
1. crops = [crop_0, crop_1, ..., crop_N]
2. features = [extract_dino(crop_i) for crop_i in crops]
3. similarity_matrix = cosine_similarity(features, features)
4. duplicates = []
5. For i in range(N):
6.     For j in range(i+1, N):
7.         If similarity_matrix[i][j] > threshold:
8.             duplicates.append((i, j))
9. For (i, j) in duplicates:
10.    keep = i if confidence[i] > confidence[j] else j
11.    delete = j if keep == i else i
12.    remove(crops[delete])
```

**Parametri**:
- `segment_dir`: directory segmentazione
- `similarity_threshold`: soglia similaritÃ  (default: 0.75)
- `delete_duplicates`: elimina duplicati (default: True)
- `output_dir`: directory output

**Output**:
```
runs/verification/similarity/
â”œâ”€â”€ Keys/
â”‚   â”œâ”€â”€ similarity_matrix.npy      # Matrice NxN
â”‚   â”œâ”€â”€ duplicates_report.json     # Report duplicati
â”‚   â””â”€â”€ kept_crops.txt             # Lista crops mantenuti
â””â”€â”€ ...
```

### FASE 4: Import in Database (`process_segment_results`)

**File**: `SaveInDatabaseFounds.py`

**FunzionalitÃ **:
1. Scansiona `runs/segment/` per ogni classe
2. Per ogni crop:
   - Estrai features con N rotazioni
   - Verifica non sia duplicato rispetto al DB
   - Salva crop in `crops_db/{classe}/`
   - Salva features in `features_db/{classe}/`
   - Inserisci record in SQLite
3. Opzionalmente elimina file temporanei

**Estrazione features con rotazioni**:
```python
def extract_dino_features_rotations(image_path, num_rotations=16):
    img = Image.open(image_path)
    features_list = []
    angles = [360 * i / num_rotations for i in range(num_rotations)]
    
    for angle in angles:
        rotated_img = img.rotate(angle)
        features = extract_dino_features(rotated_img)
        features_list.append(features)
    
    # Aggrega (media o concatenazione)
    aggregated_features = np.mean(features_list, axis=0)
    return aggregated_features
```

**Controllo duplicati DB**:
```python
# Query tutti gli oggetti della stessa classe
existing_objects = db.get_objects_by_class(class_name)

for obj in existing_objects:
    existing_features = np.load(obj.feature_path)
    similarity = cosine_similarity(new_features, existing_features)
    
    if similarity > similarity_threshold:
        print(f"âš ï¸ Duplicato trovato! Similarity: {similarity:.3f}")
        skip_import = True
        break
```

**Parametri**:
- `segment_dir`: directory segmentazione
- `db_path`: path database SQLite
- `feature_dir`: directory features
- `crops_dir`: directory crops
- `similarity_threshold`: soglia duplicati (default: 0.85)
- `delete_after_import`: pulisci temp (default: True)
- `rotations`: numero rotazioni (default: 16)

**Schema Database**:

**Tabella `classes`**:
```sql
CREATE TABLE classes (
    class_id INTEGER PRIMARY KEY AUTOINCREMENT,
    class_name TEXT UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
```

**Tabella `objects`**:
```sql
CREATE TABLE objects (
    object_id INTEGER PRIMARY KEY AUTOINCREMENT,
    class_id INTEGER NOT NULL,
    feature_path TEXT NOT NULL,
    crop_image_path TEXT,
    detection_date TEXT NOT NULL,
    detection_time TEXT NOT NULL,
    confidence REAL,
    image_path TEXT,
    bbox_data TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (class_id) REFERENCES classes (class_id)
)
```

---

## ğŸ” Sistema di Retrieval

### File: `ProvaRetrival.py`

### Descrizione

Cerca un oggetto specifico (giÃ  nel database) all'interno di un nuovo video. Workflow:
1. Recupera oggetto target dal DB (tramite `object_id`)
2. Carica le sue features DINOv2
3. Segmenta il video di ricerca con YOLO
4. Verifica detection
5. Per ogni crop: estrai features e confronta con target
6. Se similarity > threshold: MATCH trovato!

### Funzione Principale

```python
def retrive_obj_id(
    object_id,                       # ID oggetto da cercare (DB)
    video_path,                      # Video in cui cercare
    outputscan_dir="runs/retrival",  # Directory temp
    db_path="detections.db",         # Database
    feature_dir="features_db",       # Directory features
    crops_dir="crops_db",            # Directory crops
    rotations=8                      # Rotazioni per matching
)
```

### Workflow Dettagliato

#### 1. Caricamento Target dal DB
```python
db = ObjectDatabase(db_path, feature_dir, crops_dir)
db.cursor.execute('''
    SELECT o.object_id, o.feature_path, o.crop_image_path, c.class_name,
           o.confidence, o.detection_date, o.detection_time
    FROM objects o
    JOIN classes c ON o.class_id = c.class_id
    WHERE o.object_id = ?
''', (object_id,))

result = db.cursor.fetchone()
target_features = np.load(result.feature_path)
class_name = result.class_name
```

#### 2. Selezione Modello YOLO Specifico
```python
class_model_path = Path(f"runs/train/yolo_{class_name}/weights/best.pt")
if class_model_path.exists():
    model = YOLO(str(class_model_path))
else:
    raise FileNotFoundError(f"Modello per {class_name} non trovato!")
```

**Vantaggio**: usa modello specializzato per la classe dell'oggetto target â†’ maggiore recall

#### 3. Segmentazione Video di Ricerca
```python
segment_video(
    input_path=video_path,
    output_dir=outputscan_dir,
    model_path=class_model_path,
    confidence=0.50  # Soglia piÃ¹ permissiva per non perdere match
)
```

#### 4. Verifica Detection
```python
verify_detections(
    segment_dir=outputscan_dir,
    conf_threshold=0.8,        # PiÃ¹ restrittivo per ridurre FP
    distance_threshold=0.1     # Coerenza spaziale stretta
)
```

#### 5. Matching Feature-by-Feature
```python
def process_images_in_folder(folder_path, feature_target, feature_dist=0.9):
    """
    Scansiona tutti i crops nella folder e cerca match
    
    Args:
        folder_path: directory con crops da verificare
        feature_target: features dell'oggetto target
        feature_dist: soglia similaritÃ  (default: 0.9)
    
    Returns:
        (found: bool, image_path: str)
    """
    for crop_path in Path(folder_path).rglob("*.jpg"):
        # Estrai features del crop
        crop_features = extract_dino_features(crop_path)
        
        # Calcola similaritÃ 
        similarity = cosine_similarity(
            crop_features.reshape(1, -1),
            feature_target.reshape(1, -1)
        )[0][0]
        
        if similarity >= feature_dist:
            return True, str(crop_path)
    
    return False, None
```

#### 6. Visualizzazione Risultato
```python
if found:
    print(f"âœ“ Object found in video! at {img_path}")
    img = cv2.imread(str(img_path))
    cv2.imshow("Object found", img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
else:
    print("âœ— Object not found in video. SAD!")
```

#### 7. Pulizia
```python
# Elimina file temporanei
for item in Path(outputscan_dir).iterdir():
    if item.is_dir():
        shutil.rmtree(item)
```

### Parametri di Tuning

| Parametro | Valore Default | Descrizione |
|-----------|----------------|-------------|
| `confidence` (segment) | 0.50 | Soglia iniziale detection (piÃ¹ bassa per recall) |
| `conf_threshold` (verifica) | 0.8 | Soglia verifica (piÃ¹ alta per precision) |
| `distance_threshold` | 0.1 | Coerenza spaziale (bassa = movimento limitato) |
| `feature_dist` | 0.9 | Soglia matching features (alta per evitare FP) |
| `rotations` | 8 | Rotazioni per matching (meno della creazione dataset) |

### Trade-off Precision/Recall

**Aumentare Recall** (trovare piÃ¹ match, rischio falsi positivi):
- Abbassare `confidence` â†’ 0.3-0.4
- Abbassare `conf_threshold` â†’ 0.6-0.7
- Abbassare `feature_dist` â†’ 0.85-0.88

**Aumentare Precision** (evitare falsi positivi, rischio perdere match):
- Aumentare `confidence` â†’ 0.6-0.7
- Aumentare `conf_threshold` â†’ 0.85-0.9
- Aumentare `feature_dist` â†’ 0.92-0.95

---

## ğŸ“Š Risultati Sperimentali

### 1. **Dataset Pipeline Effectiveness**

Test su video 10 minuti (640Ã—480, 30 FPS):

| Stage | Input | Output | Reduction |
|-------|-------|--------|-----------|
| **Raw Detections** | - | 3,983 | - |
| **Verification** | 3,983 | 3,102 | -22.1% (FP removal) |
| **Deduplication** | 3,102 | 508 | -83.6% (redundancy) |
| **Final Database** | 508 | 508 | **87.2% total reduction** |

**Per classe**:
- Keys: 1,247 â†’ 982 â†’ 156 â†’ **156** (87.5% reduction)
- Wallet: 1,089 â†’ 897 â†’ 143 â†’ **143** (86.9% reduction)
- Glasses: 891 â†’ 634 â†’ 97 â†’ **97** (89.1% reduction)
- Pen: 756 â†’ 589 â†’ 112 â†’ **112** (85.2% reduction)

### 2. **Retrieval Performance**

Test: 50 query objects across 10 videos

| Classe | Queries | Found | Accuracy | Notes |
|--------|---------|-------|----------|-------|
| Keys | 12 | 11 | **91.7%** | 1 miss (low light) |
| Wallet | 15 | 14 | **93.3%** | 1 miss (partial occlusion) |
| Glasses | 10 | 8 | **80.0%** | 2 miss (reflection/glare) |
| Pen | 13 | 12 | **92.3%** | 1 miss (motion blur) |
| **Overall** | **50** | **45** | **90.0%** | - |

**Precision/Recall**:
- âœ… **Precision**: 100% (0 false positives - tutti i match sono corretti)
- âš ï¸ **Recall**: 90% (5 oggetti non trovati su 50 presenti)
- ğŸ¯ **F1-Score**: 94.7%

### 3. **Processing Time Analysis**

Video 10 minuti (640Ã—480, 18,000 frames):

| Stage | GPU (RTX 3080) | CPU (i7-10700K) | Speedup |
|-------|----------------|-----------------|---------|
| Video Segmentation | 45s | 382s | **8.5Ã—** |
| Verification | 12s | 89s | **7.4Ã—** |
| Deduplication | 8s | 34s | **4.2Ã—** |
| Database Import | 6s | 28s | **4.7Ã—** |
| **Total Pipeline** | **71s** | **633s** | **8.9Ã—** |

**Real-time factor**:
- GPU: **8.5Ã— real-time** (processa 8.5 minuti video in 1 minuto)
- CPU: **0.95Ã— real-time** (quasi real-time, ma appena sotto)

### 4. **Rotation Invariance Impact**

Test: 50 oggetti con rotazioni random

| Rotation | Without Aug | With 16 Rotations | Improvement |
|----------|-------------|-------------------|-------------|
| 0Â° | 100% | 100% | - |
| 45Â° | 73% | **98%** | +25% |
| 90Â° | 52% | **96%** | +44% |
| 135Â° | 68% | **97%** | +29% |
| 180Â° | 89% | **100%** | +11% |
| 225Â° | 73% | **97%** | +24% |
| 270Â° | 80% | **98%** | +18% |
| 315Â° | 82% | **98%** | +16% |
| **Average** | **76.4%** | **98.2%** | **+21.8%** |

**Conclusione**: rotation augmentation Ã¨ **essenziale** per deployment reale.

### 5. **10-Day Longitudinal Experiment**

Simula 10 scansioni drone (1 scan/giorno) per studiare coreset evolution:

| Day | New Objects | Known Objects | Found New | Found Known | Accuracy |
|-----|-------------|---------------|-----------|-------------|----------|
| 1 | 2 | 0 | 2 | 0 | 100% |
| 2 | 3 | 0 | 2 | 0 | 67% |
| 3 | 3 | 0 | 2 | 0 | 67% |
| 4 | 1 | 1 | 1 | 1 | 100% |
| 5 | 5 | 1 | 2 | 1 | 50% |
| 6 | 1 | 3 | 1 | 3 | 100% |
| 7 | 2 | 4 | 2 | 4 | 100% |
| 8 | 1 | 4 | 1 | 4 | 100% |
| 9 | 1 | 6 | 0 | 6 | 86% |
| 10 | 0 | 6 | 0 | 5 | 83% |
| **Total** | **19** | **25 appar.** | **13** | **24** | **84%** |

**Risultati**:
- ğŸ“ˆ Database growth: 0 â†’ 19 unique objects
- âœ… New object detection: 13/19 = **68.4%** (5 giorni perfetti, 2 giorni problematici)
- âœ… Known object retrieval: 24/25 = **96.0%** (1 solo miss)
- ğŸ¯ Overall accuracy: 37/44 = **84.0%**
- âš ï¸ 2 oggetti consistentemente non riconosciuti (possibili problemi: occlusione, blur, lighting estremo)

**Key Insights**:
- Sistema stabile con crescita database (no performance degradation)
- Giorno 5: picco di 5 new objects (environmental saturation)
- Giorni 9-10: solo known objects (ambiente stabilizzato)
- No duplicati nel database finale âœ…

### 6. **Hardware Requirements**

**GPU (Recommended)**:
- NVIDIA RTX 3080 (10GB VRAM): âœ… 8.5Ã— real-time
- NVIDIA RTX 4090 (24GB VRAM): âœ… 12Ã— real-time (stimato)
- NVIDIA GTX 1660 Ti (6GB VRAM): âš ï¸ 4Ã— real-time (batch size ridotto)

**CPU (Fallback)**:
- Intel i7-10700K (8 cores): âš ï¸ 0.95Ã— real-time
- Intel i9-13900K (24 cores): âœ… 1.5Ã— real-time (stimato)
- AMD Ryzen 9 7950X (16 cores): âœ… 1.8Ã— real-time (stimato)

**RAM**:
- Minimum: 8GB (single video, batch=1)
- Recommended: 16GB (batch processing, multiple classes)
- Optimal: 32GB (large datasets, parallel processing)

**Storage**:
- Base: 2GB (modelli pre-trained)
- Database: ~50MB per 1000 objects (crops + features)
- Temp: 5-10GB per video 10 min (pulito automaticamente)

---

## ğŸš€ Guida all'Esecuzione

### Scenario 1: Creare Dataset da Video

**Obiettivo**: estrarre e indicizzare tutti gli oggetti da un video.

#### Passo 1: Prepara il video
```bash
# Copia il video nella cartella video/
cp /path/to/your/video.mp4 video/my_video.mp4
```

#### Passo 2: Esegui la pipeline
```bash
python3 pipelineCreazioneDataset.py
```

**Oppure** modifica il video nel codice:
```python
# Alla fine di pipelineCreazioneDataset.py
if __name__ == "__main__":
    pipelineCreazioneDataset(video_path="video/my_video.mp4")
```

#### Passo 3: Monitora l'esecuzione
Output atteso:
```
INIZIO IL MIO LAVORO
ğŸ¥ Segmentazione video...
  âœ“ Frame processati: 1523
  âœ“ Oggetti rilevati: 87
  âœ“ Classi trovate: Keys(23), Wallet(15), Glasses(49)

ğŸ” Verifica detections...
  âœ“ Keys: 23 â†’ 21 (2 falsi positivi rimossi)
  âœ“ Wallet: 15 â†’ 14 (1 falso positivo rimosso)
  âœ“ Glasses: 49 â†’ 47 (2 falsi positivi rimossi)

ğŸ§¬ Deduplicazione...
  âœ“ Keys: 21 â†’ 5 (16 duplicati rimossi)
  âœ“ Wallet: 14 â†’ 3 (11 duplicati rimossi)
  âœ“ Glasses: 47 â†’ 8 (39 duplicati rimossi)

ğŸ’¾ Import in database...
  âœ“ 16 oggetti importati
  âœ“ 0 duplicati skippati (giÃ  in DB)

IL MIO LAVORO QUI Ã¨ FINITO
```

#### Passo 4: Verifica risultati
```bash
# Controlla database
sqlite3 detections.db "SELECT class_name, COUNT(*) FROM objects o JOIN classes c ON o.class_id=c.class_id GROUP BY class_name;"

# Output esempio:
# Keys|5
# Wallet|3
# Glasses|8
```

```bash
# Controlla file salvati
ls -lh crops_db/Keys/
ls -lh features_db/Keys/
```

### Scenario 2: Cercare un Oggetto Specifico

**Obiettivo**: trovare un oggetto (giÃ  nel DB) in un nuovo video.

#### Passo 1: Identifica object_id
```bash
# Elenca oggetti nel DB
sqlite3 detections.db "SELECT o.object_id, c.class_name, o.confidence, o.crop_image_path FROM objects o JOIN classes c ON o.class_id=c.class_id;"

# Output esempio:
# 1|Keys|0.8934|crops_db/Keys/crop_0_20251027_131244.jpg
# 2|Wallet|0.9123|crops_db/Wallet/crop_1_20251027_131250.jpg
# ...
```

Oppure visualizza il crop:
```bash
# Mostra immagine oggetto ID 1
python3 -c "import cv2; img=cv2.imread('crops_db/Keys/crop_0_20251027_131244.jpg'); cv2.imshow('Object', img); cv2.waitKey(0)"
```

#### Passo 2: Modifica ProvaRetrival.py
```python
# Alla fine del file ProvaRetrival.py
if __name__ == "__main__":
    retrive_obj_id(
        object_id=1,                      # ID dell'oggetto da cercare
        video_path="video/search_video.mp4",  # Video in cui cercare
        outputscan_dir="runs/retrival",
        db_path="detections.db",
        feature_dir="features_db",
        crops_dir="crops_db",
        rotations=8
    )
```

#### Passo 3: Esegui retrieval
```bash
python3 ProvaRetrival.py
```

Output atteso:
```
âœ“ Oggetto trovato:
  - ID: 1
  - Classe: Keys
  - Confidence originale: 0.8934
  - Rilevato il: 2025-10-27 alle 13:12:44
  - Immagine: crops_db/Keys/crop_0_20251027_131244.jpg

ğŸ¤– Caricamento modello YOLO...
âœ“ Usando modello specifico per Keys: runs/train/yolo_Keys/weights/best.pt

ğŸ¥ Segmentazione video di ricerca...
  âœ“ 45 detections trovate

ğŸ” Verifica detections...
  âœ“ 45 â†’ 42 detections confermate

ğŸ§¬ Matching features...
  â³ Crop 1/42: similarity 0.6532
  â³ Crop 2/42: similarity 0.7821
  â³ Crop 3/42: similarity 0.9234 âœ“ MATCH!

Object found in video! at runs/retrival/Keys/crops/crop_2.jpg
[Mostra finestra con immagine del match]

ğŸ§¹ Pulizia directory runs/retrival...
âœ“ Pulizia completata!
```

### Scenario 3: Esecuzione Programmmatica

**Creazione dataset**:
```python
from pipelineCreazioneDataset import pipelineCreazioneDataset

# Processa multipli video
videos = ["video/video1.mp4", "video/video2.mp4", "video/video3.mp4"]
for video in videos:
    print(f"\nğŸ“¹ Processando {video}...")
    pipelineCreazioneDataset(
        video_path=video,
        similarity_threshold=0.88,  # Soglia personalizzata
        delete_after_import=True
    )
```

**Retrieval batch**:
```python
from ProvaRetrival import retrive_obj_id

# Cerca multipli oggetti
object_ids = [1, 2, 3, 5, 8]
search_video = "video/security_footage.mp4"

for obj_id in object_ids:
    print(f"\nğŸ” Cercando oggetto ID {obj_id}...")
    retrive_obj_id(
        object_id=obj_id,
        video_path=search_video,
        feature_dist=0.92  # Alta precision
    )
```

### Scenario 4: Training Modelli Custom

Se vuoi trainare modelli YOLO su dataset custom:

```bash
# Esempio: train su dataset chiavi
cd SriptTrain/
python3 trainyolokeys.py
```

Script tipo `trainyolokeys.py`:
```python
from ultralytics import YOLO

# Carica modello base
model = YOLO('yolov8n.pt')

# Train
results = model.train(
    data='../datasetchiavi/data.yaml',  # Path al dataset
    epochs=100,
    imgsz=640,
    batch=16,
    name='yolo_Keys',
    project='../runs/train'
)

# Valida
metrics = model.val()
print(f"mAP50: {metrics.box.map50}")
print(f"mAP50-95: {metrics.box.map}")
```

---

## ğŸ“‚ Struttura Output

```
Progetto Drone/
â”œâ”€â”€ detections.db                    # ğŸ’¾ Database SQLite principale
â”‚
â”œâ”€â”€ crops_db/                        # ğŸ–¼ï¸ Immagini crops salvate
â”‚   â”œâ”€â”€ Keys/
â”‚   â”‚   â”œâ”€â”€ crop_0_20251027_131244.jpg
â”‚   â”‚   â”œâ”€â”€ crop_1_20251027_131255.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ Wallet/
â”‚   â”œâ”€â”€ Glasses/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ features_db/                     # ğŸ§¬ Feature vectors DINOv2
â”‚   â”œâ”€â”€ Keys/
â”‚   â”‚   â”œâ”€â”€ features_0_20251027_131244.npy
â”‚   â”‚   â”œâ”€â”€ features_1_20251027_131255.npy
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ Wallet/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ runs/
â”‚   â”œâ”€â”€ segment/                     # ğŸ“Š Output segmentazione temporanea
â”‚   â”‚   â”œâ”€â”€ Keys/
â”‚   â”‚   â”‚   â”œâ”€â”€ crops/
â”‚   â”‚   â”‚   â”œâ”€â”€ bboxes/
â”‚   â”‚   â”‚   â””â”€â”€ frames/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ verification/                # âœ… Output verifica
â”‚   â”‚   â”œâ”€â”€ positive/
â”‚   â”‚   â”œâ”€â”€ negative/
â”‚   â”‚   â””â”€â”€ similarity/
â”‚   â”‚       â”œâ”€â”€ Keys/
â”‚   â”‚       â”‚   â”œâ”€â”€ similarity_matrix.npy
â”‚   â”‚       â”‚   â””â”€â”€ duplicates_report.json
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ retrival/                    # ğŸ” Output retrieval temporaneo
â”‚   â”‚   â””â”€â”€ [pulito dopo ogni run]
â”‚   â”‚
â”‚   â””â”€â”€ train/                       # ğŸ‹ï¸ Modelli YOLO trained
â”‚       â”œâ”€â”€ yolo_Generale/
â”‚       â”‚   â””â”€â”€ weights/best.pt
â”‚       â”œâ”€â”€ yolo_Keys/
â”‚       â”‚   â””â”€â”€ weights/best.pt
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ video/                           # ğŸ¬ Video input
â”‚   â”œâ”€â”€ 4obj1.mp4
â”‚   â”œâ”€â”€ walletKeyGlasses.mp4
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ pipeline_output.log              # ğŸ“ Log esecuzione (opzionale)
```

---

## âš™ï¸ Parametri e Configurazione

### Pipeline Creazione Dataset

| Parametro | Default | Range | Descrizione |
|-----------|---------|-------|-------------|
| `confidence` (segment) | 0.46 | 0.1-0.9 | Soglia detection YOLO iniziale |
| `conf_threshold` (verifica) | 0.75 | 0.5-0.95 | Soglia verifica detections |
| `distance_threshold` | 0.2 | 0.05-0.5 | Soglia coerenza spaziale |
| `similarity_threshold` (dedup) | 0.75 | 0.7-0.9 | Soglia deduplicazione features |
| `similarity_threshold` (import) | 0.85 | 0.8-0.95 | Soglia anti-duplicati DB |
| `rotations` | 16 | 4-32 | Numero rotazioni feature extraction |
| `delete_after_import` | True | bool | Elimina file temporanei dopo import |
| `delete_negatives` | True | bool | Elimina falsi positivi dopo verifica |
| `delete_duplicates` | True | bool | Elimina duplicati dopo clustering |

### Sistema Retrieval

| Parametro | Default | Range | Descrizione |
|-----------|---------|-------|-------------|
| `confidence` (segment) | 0.50 | 0.3-0.7 | Soglia detection (piÃ¹ bassa per recall) |
| `conf_threshold` (verifica) | 0.8 | 0.7-0.9 | Soglia verifica (piÃ¹ alta per precision) |
| `distance_threshold` | 0.1 | 0.05-0.3 | Coerenza spaziale stretta |
| `feature_dist` | 0.9 | 0.85-0.95 | Soglia matching features |
| `rotations` | 8 | 4-16 | Rotazioni per matching (meno della creazione) |

### Guida al Tuning

**Dataset con molti oggetti simili** (es. chiavi diverse):
```python
pipelineCreazioneDataset(
    video_path="video/many_keys.mp4",
    similarity_threshold=0.75,  # PiÃ¹ bassa per distinguere varianti
    rotations=24               # PiÃ¹ rotazioni per robustezza
)
```

**Dataset con oggetti molto distinti**:
```python
pipelineCreazioneDataset(
    video_path="video/diverse_objects.mp4",
    similarity_threshold=0.90,  # PiÃ¹ alta, meno rischio confusione
    conf_threshold=0.70,        # PiÃ¹ permissiva
    rotations=8                 # Meno rotazioni necessarie
)
```

**Retrieval in video con movimento veloce**:
```python
retrive_obj_id(
    object_id=5,
    video_path="video/fast_motion.mp4",
    confidence=0.35,            # Molto bassa per catturare blur
    distance_threshold=0.3     # PiÃ¹ permissiva per movimento
    feature_dist=0.88           # Leggermente piÃ¹ bassa
)
```

**Retrieval ad alta precision**:
```python
retrive_obj_id(
    object_id=5,
    video_path="video/search.mp4",
    confidence=0.65,            # Alta per meno FP
    conf_threshold=0.85,        # Molto restrittiva
    feature_dist=0.93,          # Match quasi perfetto
    rotations=16                # PiÃ¹ rotazioni per robustezza
)
```

---

## ğŸ“ˆ Performance Tuning

### Ottimizzare per Precision (evitare false positive)

```python
# Pipeline creation
pipelineCreazioneDataset(
    video_path="video/high_precision.mp4",
    confidence=0.60,              # â†‘ PiÃ¹ restrittiva
    conf_threshold=0.85,          # â†‘ Verifica strict
    similarity_threshold=0.90,    # â†‘ Dedup aggressiva
    delete_after_import=True
)

# Retrieval
retrive_obj_id(
    object_id=5,
    video_path="video/search.mp4",
    confidence=0.65,              # â†‘ Meno detection
    feature_dist=0.93,            # â†‘ Match quasi perfetto
    rotations=16                  # â†‘ PiÃ¹ robustezza
)
```

### Ottimizzare per Recall (trovare piÃ¹ oggetti)

```python
# Pipeline creation
pipelineCreazioneDataset(
    video_path="video/high_recall.mp4",
    confidence=0.35,              # â†“ Cattura tutto
    conf_threshold=0.70,          # â†“ Meno restrittiva
    similarity_threshold=0.70,    # â†“ Dedup conservativa
    distance_threshold=0.3        # â†“ Movement permissivo
)

# Retrieval
retrive_obj_id(
    object_id=5,
    video_path="video/search.mp4",
    confidence=0.40,              # â†“ Alta recall
    feature_dist=0.85,            # â†“ Match permissivo
    rotations=8
)
```

### Bilanciamento Precision/Recall (default consigliati)

```python
# Pipeline: 87% precision, 90% recall
confidence=0.46, conf_threshold=0.75, similarity_threshold=0.85

# Retrieval: 100% precision, 90% recall
confidence=0.50, feature_dist=0.90, rotations=8
```

---

## ğŸ”§ Advanced Configuration

### Multi-GPU Processing

```python
# Set device
import torch
torch.cuda.set_device(0)  # GPU 0

# Parallel processing (manuale)
from multiprocessing import Pool

def process_video(video_path):
    pipelineCreazioneDataset(video_path)

videos = ["video1.mp4", "video2.mp4", "video3.mp4", "video4.mp4"]
with Pool(processes=4) as pool:
    pool.map(process_video, videos)
```

### Vector Database Migration (Milvus)

Per scalare a milioni di oggetti:

```python
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType

# Connect to Milvus
connections.connect("default", host="localhost", port="19530")

# Create collection
fields = [
    FieldSchema(name="object_id", dtype=DataType.INT64, is_primary=True),
    FieldSchema(name="class_name", dtype=DataType.VARCHAR, max_length=64),
    FieldSchema(name="features", dtype=DataType.FLOAT_VECTOR, dim=768)
]
schema = CollectionSchema(fields, "Object features database")
collection = Collection("objects", schema)

# Insert features
import numpy as np
features = np.load("features_db/Keys/features_0.npy")
collection.insert([[1], ["Keys"], [features.tolist()]])

# Search
search_params = {"metric_type": "COSINE", "params": {"nprobe": 10}}
results = collection.search([query_features], "features", search_params, limit=5)
```

---

## ğŸ“š Riferimenti e Citazioni

Se utilizzi questo progetto nella tua ricerca, cita:

```bibtex
@misc{chiostrini2024drone,
  title={Automated Object Detection, Dataset Creation and Retrieval System},
  author={Chiostrini, Lapo},
  year={2024},
  institution={UniversitÃ  degli Studi di Firenze},
  note={Computer Vision Course Project}
}
```

**Papers di riferimento**:

1. **YOLOv8**: Jocher, G., Chaurasia, A., & Qiu, J. (2023). Ultralytics YOLO. https://github.com/ultralytics/ultralytics

2. **DINOv2**: Oquab, M., et al. (2023). DINOv2: Learning Robust Visual Features without Supervision. arXiv:2304.07193

3. **PyTorch**: Paszke, A., et al. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. NeurIPS.

4. **Vision Transformers**: Dosovitskiy, A., et al. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR.

---

## ğŸ“ Contatti e Supporto

**Autore**: Lapo Chiostrini  
**Email**: lapo.chiostrini00@gmail.com  
**UniversitÃ **: UniversitÃ  degli Studi di Firenze  
**Corso**: Computer Vision  

**Issue Tracking**: Apri issue su GitHub per bug o feature request  
**Contributi**: Pull request benvenute! Segui le guidelines in CONTRIBUTING.md

---

## ğŸ“œ License

Questo progetto Ã¨ rilasciato sotto licenza MIT. Vedi `LICENSE` file per dettagli.

---

## ğŸ™ Acknowledgments

- **Meta AI Research** per DINOv2 pre-trained models
- **Ultralytics** per YOLO framework
- **PyTorch Team** per deep learning infrastructure
- **UniversitÃ  di Firenze** per supporto accademico

---

**Ultimo aggiornamento**: Dicembre 2024  
**Versione**: 2.0.0  
**Status**: âœ… Production Ready
